{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMBD-Project-3\n",
    "\n",
    "## **Question 1:**\n",
    "\n",
    "For a metric in deciding whether a change in the algorithm provides an improvement or not, we decided that using an \n",
    "accuracy function would be most appropriate. After investigating, we found the purity accuracy function [[1]](#1):\n",
    "\n",
    "$$Pur(Q,Y) = \\frac{1}{N}\\sum_{k=1}^K max_{j=1...c} |Q_k \\cap Y_j |$$\n",
    "\n",
    "Where $Q$ is the predicted class and $Y$ is the expected class. Essentially for each assigned cluster, we identify the most\n",
    "common item in the cluster and assign the cluster the value of this item. We then get the accuracy by \n",
    "dividing the correctly clustered items with the total number of clusters. \n",
    "\n",
    "We chose an external measurement as they are easier to interpreter than the internal metrics mentioned in the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 2:**\n",
    "After investigating various learning rate functions suggested by W. Natita et. al. [[2]](#2), we used the following learning \n",
    "rate functions in addition to the default function:\n",
    "\n",
    "$$ \\alpha(t,T) = \\alpha(0) \\times \\exp\\left(\\frac{-t}{T}\\right) \\tag{default} $$\n",
    "\n",
    "$$ \\alpha(t) = \\alpha(0) \\times \\frac{1}{t} \\tag{linear}$$\n",
    "\n",
    "$$ \\alpha(t,T) = \\alpha(0) \\times \\left(1 - \\frac{t}{T}\\right) \\tag{inverse of time}$$\n",
    "\n",
    "$$ \\alpha(t,T) = \\alpha(0) \\times \\exp{\\left(\\frac{t}{T}\\right)} \\tag{power series} $$\n",
    "\n",
    "$T$ is the total number of iterations (epochs), and $t$ is the current iteration. These changes were integrated into the $decay\\_learning\\_rate$ functions specified above. We also investigated changing the initial learning rate by considering varying number of epochs in **[10, 50, 100, 300, 400, 500]** to better see the effect of the learning rate and the function changes. The learning rates we tested were **[1, 0.1, 0.001, 0.0001]**.\n",
    "\n",
    "In summary,\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Rank  |  Accuracy | Learning Function  | Learning Rate | Epochs\n",
    "|---|---|---|---|---|\n",
    "| 1st Highest  |  91\\% | power  | 0.01  | 300\n",
    "|---|---|---|---|---|\n",
    "| 2nd Highest  | 90\\%  | default & inverse |  1.0 \\& 0.001 | 300 \\& 100\n",
    "|---|---|---|---|---|\n",
    "| Lowest | 68\\%  |  linear | 0.001  | 500  |\n",
    "\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "> Default\n",
    "\n",
    "- With the default learning function, we observed a marginal increase in accuracy using smaller learning rates across all epochs.\n",
    "- we observed a study increase in accuracy across epochs until 300 epochs where we recorded the highest.\n",
    "- at learning rates 0.1 and 0.01, we do not observe any significant change in accuracy across the epochs considered\n",
    "- with learning rate 0.001 we recorded second highest accuracy of 90\\% for 300 iterations which later dropped to 78\\% at 500 epochs.\n",
    "\n",
    "> Linear Function\n",
    "\n",
    "- The overall trend of accuracy tends to be decreasing with smaller learning rates, holding an epoch constant, \n",
    "- At learning rate 1, after 50 iteractions and more, the linear learning function does not produce any change in accuracy across all epochs.\n",
    "- we realize a sudden increase in accuracy from 80\\% to 86\\% across the epochs at learning rate 0.1.\n",
    "- we see an oscilating behaviour across epochs at learning rate 0.01 with 82\\% minimum and 89\\% maximum.\n",
    "- Finally, at 0.001, the trend in accuracy seems to be decreasing across the epochs from 88\\% at 50 epoch to 68\\% at 500 epochs.\n",
    "\n",
    "> Inverse Function\n",
    "\n",
    "- keeping constant the epochs, we observe significant improvements in accuracy measure using smaller learning rates.\n",
    "- At learning rate of 1.0, we see a steady decrease in the accuracy falling from 84\\% at 50 epochs to 72\\% at 500 epochs.\n",
    "- With learning rate 0.1, there is a gradual decrease in accuracy from 86\\% to 85\\% across the epochs.\n",
    "- Changing the number of epochs resulted in alot of improvements in the accuracy values recorded, although we had a drop at 100 epoch from 82\\% to 76\\%, the accuracy remained constant after hitting back at 86\\%.\n",
    "- After rising from 88\\% to 90\\% at 100 epochs using learning rate of 0.001, the accuracy remained constant steadily constant after dropping to 86\\%.\n",
    "\n",
    "> Power Function\n",
    "\n",
    "- Using this function recorded the highest accuracy recorded. \n",
    "- using smaller learning rates for all epochs guarantees improvements.\n",
    "- This function performed well on our training set compared to the others guranteeing a steady improvement.\n",
    "\n",
    "\n",
    "From this we can see that improvements are obtainable by using different learning rate functions, however some of them perform better than others. \n",
    "\n",
    "\n",
    "http://www.cis.hut.fi/somtoolbox/documentation/somalg.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 3**\n",
    "We interpreted the curve of distribution as being the radius of the neighbourhood function. We investigated the use of \n",
    "removing the decreasing radius function, and also providing a fixed decreasing radius value of 0.5. We were not able to \n",
    "find substitutes to this function. all the tests were run with 40 epochs as this shows the best variation in the data. \n",
    "It was interesting to note the fixed decreasing radius had better results in 60 epochs than the dynamic decreasing radius.\n",
    "Not changing the radius performed badly as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question 4**\n",
    "We interpreted the normal distribution as the neighborhood function $h_{ck}(t)$. After investigating we found various possible neighborhood functions namely gaussian(the default one), bubble [[2]](#2), mexican-hat [[3]](#3) and triangular neighborhood.\n",
    "We selected the mexican-hat neighborhood function as we were not able to understand how to implement the simple bubble \n",
    "function. The mexican-hat function is defined as [[3]](#3):\n",
    "\n",
    "$$h_{ck}(t) =(1-2\\frac{||r_k-r_c||^2}{\\sigma(t)^2}) \\times e^{-\\frac{||r_k-r_c||^2}{\\sigma(t)^2}}$$\n",
    "\n",
    "With $x$ and $c$ representing an input pattern and centre, and width being $2w$. \n",
    "\n",
    "We also decided to investigate some changes to the gaussian neighbourhood function[4]:\n",
    "\n",
    "$$h_{ck}(t) = e^{-q\\frac{||r_k-r_c||^p}{\\sigma(t)^2}}$$\n",
    "\n",
    "Where the values of $p$ were  2 and 3, and the values of $q$ were 0.1,1 and 2.\n",
    "\n",
    "When comparing the 2 base functions, we tested different ranges of epochs: 60,80,100,150 and 200.\n",
    "\n",
    "We found that using a different neighbourhood function did not lead to a better result, however the graphs of the \n",
    "mexican-hat function looks slightly worse, due to the fact that there are more clusters with elements in. This is a \n",
    "weakness in the metric we picked in Question 1. \n",
    "\n",
    "https://www.intechopen.com/chapters/69305"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "<a id=1>[1]</a> F. Forest, M. Lebbah, H. Azzag, and J. Lacaille, ‘A Survey and Implementation of Performance Metrics for Self-Organized Maps’, arXiv:2011.05847 [cs], Nov. 2020, Accessed: Jan. 30, 2022. [Online]. Available: http://arxiv.org/abs/2011.05847\n",
    "\n",
    "<a id=1>[2]</a> Mathematics Department, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand, W. Natita, W. Wiboonsak, and S. Dusadee, ‘Appropriate Learning Rate and Neighborhood Function of Self-organizing Map (SOM) for Specific Humidity Pattern Classification over Southern Thailand’, IJMO, vol. 6, no. 1, pp. 61–65, 2016, doi: 10.7763/IJMO.2016.V6.504.\n",
    "\n",
    "<a id=1>[3]</a> https://coursepages2.tuni.fi/tiets07/wp-content/uploads/sites/110/2019/01/Neurocomputing3.pdf\n",
    "\n",
    "<a id=1>[4]</a> L. A. Tu, Improving Feature Map Quality of SOM Based on Adjusting the Neighborhood Function. IntechOpen, 2019. doi: 10.5772/intechopen.89233."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d006a4e8270f7cb3dd13063a6e39ae95bb6952aca649802565f92bfae9aa0161"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
